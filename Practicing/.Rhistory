head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
acc <- mean(knnResult == iris$Species[nw])
acc
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=1               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK1 <- mean(knnResult == iris$Species[nw])
accK1
# Drawing 50 random iris observations to train the knn ML model
set.seed(12L) # Specifies a seed
tr <- sample (150,50) # Creates training set of 50 points of data
nw <- sample (150,50) # Creates new set of data to determine accuracy of ML model
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=1               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK1 <- mean(knnResult == iris$Species[nw])
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=2               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK1 <- mean(knnResult == iris$Species[nw])
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=1               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK1 <- mean(knnResult == iris$Species[nw])
accK1
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=2               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK2 <- mean(knnResult == iris$Species[nw])
accK1
accK2
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=3               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK3 <- mean(knnResult == iris$Species[nw]) # Modified to check different values of k
accK1 # accuracy when k = 1
accK2 # accuracy when k = 2
acck3 # accuracy when k = 3
acck3 # accuracy when k = 3
accK3 <- mean(knnResult == iris$Species[nw]) # Modified to check different values of k
accK1 # accuracy when k = 1
accK2 # accuracy when k = 2
acck3 # accuracy when k = 3
accK3 # accuracy when k = 3
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=4               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK4 <- mean(knnResult == iris$Species[nw]) # Modified to check different values of k
accK1 # accuracy when k = 1
accK2 # accuracy when k = 2
accK3 # accuracy when k = 3
accK4 # accuracy when k = 4
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=5               # Assigns number of neighbors to check
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK5 <- mean(knnResult == iris$Species[nw]) # Modified to check different values of k
accK1 # accuracy when k = 1; 0.96 for the above seed
accK2 # accuracy when k = 2; 0.94 for the above seed
accK3 # accuracy when k = 3; 0.94 for the above seed
accK4 # accuracy when k = 4; 0.96 for the above seed
accK5 # accuracy when k = 5;
knnResult <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=5,              # Assigns number of neighbors to check
prob = TRUE
) # Runs the ML model
head(knnResult) # See the first part of the factor after running the knn function
# Compare observed knn product to the observed outcome
table(knnResult, iris$Species[nw]) # Compares the Species printed by the knn function to the expected species
accK5 <- mean(knnResult == iris$Species[nw]) # Modified to check different values of k
accK1 # accuracy when k = 1; 0.96 for the above seed
accK2 # accuracy when k = 2; 0.94 for the above seed
accK3 # accuracy when k = 3; 0.94 for the above seed
accK4 # accuracy when k = 4; 0.96 for the above seed
accK5 # accuracy when k = 5; 0.94 for the above seed
knnResult5Prob <- knn(iris[tr, -5],     # Assigns training data
iris[nw,-5],      # Assigns testing data
iris$Species[tr], # Assigns response variables
k=5,              # Assigns number of neighbors to check
prob = TRUE
) # Runs the ML model
head(knnResult5Prob) # See the first part of the factor after running the knn function
# Checking what happens with 'prob = true'
table(attr(knnResult5Prob, "prob"))
?attr
?lm
?data
?lm
?~
help
?.
??.
?lm
?predict
data(diamonds) # loads the data set
# Load the Caret library to use the data within
library("caret")
data(diamonds) # loads the data set
model <- lm(price ~ ., diamonds) # Linear model where "price ~ ." is the formula
p <- predict(model, diamonds) # Generic prediction for model fitting functions
# Load the Caret library to use the data within
library("caret")
data(diamonds) # loads the data set
model <- lm(price ~ ., diamonds) # Linear model where "price ~ ." is the formula
p <- predict(model, diamonds) # Generic prediction for model fitting functions
# In-sample error on prediction
error <- p - diamonds$price
rmse_in <- sqrt(mean(error^2)) # in-sample RMSE
rmse_in
source("C:/Users/autum/Sites/AuqaMachina/Practicing/regressionPractice.R")
?nrow
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
n_test <- nrow(diamonds) * 0.80 # Pull 80% of rows out for testing
test <- sample(nrow(diamonds), ntest)
?sample
View(diamonds)
?lm
# Load the Caret library to use the data within
library("caret")
data(diamonds) # loads the data set
model1 <- lm(price ~ ., diamonds) # Linear model excluding price from the data examined
# Load the Caret library to use the data within
library("caret")
data(diamonds) # loads the data set
model1 <- lm(price ~ ., diamonds) # Linear model excluding price from the data examined
p1 <- predict(model1, diamonds) # Generic prediction for model fitting functions
# In-sample error on prediction
error1 <- p1 - diamonds$price
rmse_in <- sqrt(mean(error1^2)) # in-sample RMSE
rmse_in
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
n_test <- nrow(diamonds) * 0.80 # Calculate the number for 80% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(42) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
n_test <- nrow(diamonds) * 0.80 # Calculate the number for 80% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
per <- 0.80
function (data, expr, ...)
# Load the Caret library to use the data within
library("caret")
data(diamonds) # loads the data set
model1 <- lm(price ~ ., diamonds) # Linear model excluding price from the data examined
p1 <- predict(model1, diamonds) # Generic prediction for model fitting functions
# In-sample error on prediction
error1 <- p1 - diamonds$price
rmse_in <- sqrt(mean(error1^2)) # in-sample RMSE
rmse_in
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
per <- 0.80
n_test <- nrow(diamonds) * per # Calculate the number for [percent]% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
per <- 0.90
n_test <- nrow(diamonds) * per # Calculate the number for [percent]% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
per <- 0.50
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
per <- 0.50
n_test <- nrow(diamonds) * per # Calculate the number for [percent]% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
# Create out of sample RMSE by removing 20% to serve as testing data
set.seed(29) # Choose a seed to keep same 20% of samples excluded to play with other parameters of model
per <- 0.95
n_test <- nrow(diamonds) * per # Calculate the number for [percent]% of the rows
test <- sample(nrow(diamonds), n_test) # Create a dataframe with 80% of the rows of diamonds to serve as the testing data
model2 <- lm(price ~ ., data = diamonds[test, ]) # Create a linear model using the same set up as before, but with less training data
p2 <- predict(model2, diamonds[-test, ]) # Predict the price of the testing data based on the linear model
# Calculate out-of-sample RMSE using the 20% for testing
error2 <- p2 - diamonds$price[-test] # calculate the difference in the predicted prices from the actual prices
rmse_out <- sqrt(mean(error2^2))
rmse_out
rmse_in
?trControl
?trainControl
# Load the Caret library to use the data within
library("caret") # contains 'diamonds' data to be used, and 'train' function used to create folds
# Creating the folds and model to be used
set.seed(29)
model <- train(price ~ ., diamonds, # Train the model with the diamonds data set and use price as the output
method = "lm", #linear model
trControl = trainControl(method = "cv",      # Resampling method
number = 10,        # Number of folds
verboseIter = FALSE # verboseIter determines if the train function prints its iterations
)
)
model
model
model
# Using the model to predict the price of diamonds
p <- predict(model, diamonds)
error <- p - diamonds$price
rmse_xval <- sqrt(mean(error^2)) # Cross validated RMSE
rmse_xval
library("MASS") # Contains data on Boston homes to use as practice
Boston
Boston
Bos <- Boston
View(Bos)
# Load the Caret library to use the data within
library("caret") # contains 'diamonds' data to be used, and 'train' function used to create folds
library("MASS")  # Contains data on Boston homes to use as practice
# Creating the folds and model to be used for diamond data
set.seed(29)
model <- train(price ~ ., diamonds, # Train the model with the diamonds data set and use price as the output
method = "lm", #linear model
trControl = trainControl(method = "cv",      # Resampling method
number = 10,        # Number of folds
verboseIter = FALSE # verboseIter determines if the train function prints its iterations
)
)
model
# Using the model to predict the price of diamonds
p_dia <- predict(model, diamonds)
error_dia <- p_dia - diamonds$price
rmse_xval_dia <- sqrt(mean(error_dia^2)) # Cross validated RMSE
rmse_xval_dia
# Creating the folds and model to be used for diamond data
set.seed(29)
model_dia <- train(price ~ ., diamonds, # Train the model with the diamonds data set and use price as the output
method = "lm", #linear model
trControl = trainControl(method = "cv",      # Resampling method
number = 10,        # Number of folds
verboseIter = FALSE # verboseIter determines if the train function prints its iterations
)
)
# Using the model to predict the price of diamonds
p_dia <- predict(model, diamonds)
# Using the model to predict the price of diamonds
p_dia <- predict(model_dia, diamonds)
error_dia <- p_dia - diamonds$price
rmse_xval_dia <- sqrt(mean(error_dia^2)) # Cross validated RMSE
# Using the model to predict the price of diamonds
p_dia <- predict(model_dia, diamonds)
error_dia <- p_dia - diamonds$price
rmse_xval_dia <- sqrt(mean(error_dia^2)) # Cross validated RMSE
rmse_xval_dia
# Creating the folds and model to be used for Boston home data
set.seed(29)
model_Bos <- train(medv ~ ., Boston, # Train the model with the diamonds data set and use price as the output
method = "lm", #linear model
trControl = trainControl(method = "cv",      # Resampling method
number = 10,        # Number of folds
verboseIter = FALSE # verboseIter determines if the train function prints its iterations
)
)
# Using the model to predict the price of diamonds
p_Bos <- predict(model_Bos, Boston)
error_Bos <- p_Bos- Boston$medv
rmse_xval_Bos <- sqrt(mean(error_Bos^2)) # Cross validated RMSE
rmse_xval_Bos
library("modeldata") # Contains the 'mlc_churn' data used for practice
library("C50") # Contains churn data
library("caret") # Using the 'train' function
library("glmnet") # Required to use the glmnet method
library("Matrix") # Required to use the glmnet method
data(mlc_churn) # Loads in the data used for this practice
set.seed(43) # Set a specific seed to prevent randomness generating a new set of samples
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
my_folds <- createFolds(churnTrain$churn,
k = 5) # Defines the number of folds
str(my_folds) # lists out the folds
sapply(my_folds, function(i){ # Verify that the folds have a similar proportion of the yes/no results
table(churnTrain$churn[i])/length(i)
})
my_control <- trainControl(summaryFunction = twoClassSummary, # The output of the training will have two different options
classProb = TRUE, # recalculate class probabilities for each re-sample
savePredictions = TRUE, # All hold-out predictions for each re-sample should be saved
index = my_folds # Lists each element for each re-sampling iteration
)
# Create the glm model
glm_model <- train(churn ~ ., # Determines the output
churnTrain, # All of the training data
metric = "ROC", # Use ROC to determine which classification threshold is best
method = "glmnet", # Use a generelized linear regression method for this model
tuneGrid = expand.grid( # Data frame with tuning values
alpha = 0:1,# One of the tuning parameters for glmnet, mixing percentage
lambda = 0:10/10), # The other tuning parameter for glmnet, regularization parameter
trControl = my_control # Sets training controls to pre-established parameters
)
print(glm_model)
glm_plot <- plot(glm_model)
glm_plot
# Developing a random forest model
rf_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "ranger", # Random forest method
tuneGrid = expand.grid(
mtry = c(2, 5, 10, 19), # Number of randomly selected predictors, max being 20
splitrule = c("gini", "extratrees"), # Defines the splitting rules as gini and extra trees
min.node.size = 1 # Sets minimum node size to 1
),
trControl = my_control # Sets the training controls as the pre-established parameters
)
print(rf_model)
rf_plot <- plot(rf_model)
rf_plot
# Developing a kNN model
knn_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # same metric as before
method = "knn", # kNN method
tuneLength = 20, # Number of tuning parameters, 20 because the data contains 20 columns
trControl = my_control)
print(knn_model)
knn_plot <- plot(knn_model)
knn_plot
names(getModelInfo())
# Create a Support Vector Machine (svm) model, using a linear basis first
svml_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "svmLinear", # Will do a radial one next
tuneLength = 20, # Selected bc that's the number of columns in the data
trControl = my_control
)
# install.packages("C50")
# install.packages("modeldata")
# install.packages("glmnet")
# install.packages("Matrix")
# install.packages("caret")
install.packages("kernlab")
# Create a Support Vector Machine (svm) model, using a linear basis first
svml_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "svmLinear", # Will do a radial one next
tuneLength = 20, # Selected bc that's the number of columns in the data
trControl = my_control
)
print(svml_model)
svml_plot <- plot(svml_model)
# Create a Support Vector Machine (svm) model, using a linear basis first
svm_lin_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "svmLinear", # Will do a radial one next
tuneGrid = expand.grid(
C = c(0.25, 0.5, 1, 5, 8, 12, 100) # Values of C randomly selected, can be refined later if this method is chosen
),
trControl = my_control
)
print(svm_lin_model)
svm_lin_plot <- plot(svm_lin_model)
svm_lin_plot
# Create a Support Vector Machine (svm) model, using a linear basis first
svm_lin_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "svmLinear", # Will do a radial one next
tuneGrid = expand.grid(
C = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100) # Values of C randomly selected, can be refined later if this method is chosen
),
trControl = my_control
)
print(svm_lin_model)
svm_lin_plot <- plot(svm_lin_model)
svm_lin_plot
# Create an svm radial model
svm_rad_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same as before
method = "svmRadial", # Radial basis for svm rather than linear
tuneLength = 10, # randomly selected
trControl = myControl
)
# Create an svm radial model
svm_rad_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same as before
method = "svmRadial", # Radial basis for svm rather than linear
tuneLength = 10, # randomly selected
trControl = my_control
)
print(svm_rad_model)
svm_rad_plot <- plot(svm_rad_model)
svm_rad_plot
names(getModelInfo())
# Create a Naive-Bayes model
nb_model <- train(churn ~ .,
churnTrain,
metric = "ROC",
method = "naive_bayes", # Naive bayes model type
trControl = my_control
)
# install.packages("C50")
# install.packages("modeldata")
# install.packages("glmnet")
# install.packages("Matrix")
# install.packages("caret")
# install.packages("kernlab")
install.packages("naivebayes")
# Create a Naive-Bayes model
nb_model <- train(churn ~ .,
churnTrain,
metric = "ROC",
method = "naive_bayes", # Naive bayes model type
trControl = my_control
)
print(nb_model)
nb_plot <- plot(nb_model)
nb_plot
?resamples
# Comparing all the different models
model_list <- list(glmnet = glm_model,
rf = rf_model,
knn = knn_model,
svm_lin = svm_lin_model,
svm_rad = svm_rad_model,
nb = nb_model)
resamp = resamples(model_list) # collects, analyzes, and visualizes a set of resampling results from a common data set, ie churnTest
print(resamp)
summary(resamp)
lattice::bwplot(reamp, metric = "ROC")
lattice::bwplot(resamp, metric = "ROC")
?preProcess
# Using model on set aside test data
p <- predict(rf_model, churnTest)
# Using model on set aside test data
p <- predict(rf_model, churnTest)
confusionMatrix(p, churnTest$churn)
load("C:/Users/autum/Sites/AuqaMachina/Practicing/.RData")
