col_types = cols(.default = col_character())) %>% # Read in the file
# Rename columns to include units
rename("Date" = "Rainfall/Runoff event",   # Date for ease of joining later
"Cl(-) [ug/L]" = "Cl(-)",           # Chloride concentration
"SO4(2-) [ug/L]" = "SO4 (2-)",      # Sulfate concentration
"PO4(3-) [ug/L]" = "PO4 (3-)",      # Phosphate concentration
"NH3-N [ug/L]" = "NH3-N",           # Ammonia concentration
"NO3(-)-N [ug/L]" = "NO3(-)-N",     # Nitrate concentration
"TDN [ug/L]" = "TDN",               # Total dissolved nitrogen
"COD [mg/L]" = "COD",               # Chemical oxygen demand
"Turbidity [NTU]" = "Turbidity",    # Turbidity
"NO2(-)-N [ug/L]" = "NO2(-)-N") %>% # Nitrite concentration
slice(-1) # Remove the row containing units
# Read in RainMeanCharacteristics2.csv
rain_chara_2.df <- read_csv("Data/commaSeparatedValues/RainMeanCharacteristics2.csv",
col_types = cols(.default = col_character())) %>%  # Units were already included in top row, so full renaming is not necessary
rename("Date" = `Rainfall/Runoff event`)
rain_chara_full.df <- full_join(rain_chara_1.df, rain_chara_2.df, by = "Date")
View(rain_chara_full.df)
inf_phys_chem.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) #%>% # Reads the data set
View(inf_phys_chem.df)
?select
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select("^Avg")
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select("^Avg .+")
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select("^Avg .*")
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(regex("^Avg .*"))
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(str_extract("^Avg .*"))
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(str_extract("^Avg "))
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(matches("^Avg"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Rainfall/Runoff Characteristics
RF_RO_chara.df <- read_csv('Data/commaSeparatedValues/RF_RO_Event_Characteristics.csv',
col_types = cols(.default = col_character())) %>%
# Rename columns to include units
rename("PHD [hr]" = "PHD",                  # Previous dry hours
"d_rain [min]" = "d_rain",           # Duration of the rain event
"h_rain [in]" = "h_rain",            # Depth of the rain event
"i_rain-max [mm/hr]" = "i_rain-max", # Rainfall maximum intensity
"IPRT [min]" = "IPRT",               # Initial pavement residence time
"V_runoff [L]" = "V_runoff",         # Volume of runoff
"C [-]" = "C",                       # Runoff coefficient
"Q_inf-max [L/s]" = "Qinf-max",      # Maximum influent flow rate
"Q_inf-avg [L/s]" = "Qinf-avg",      # Average influent flow rate
"Q_eff-max [L/s]" = "Qeff-max",      # Maximum effluent flow rate
"Q_eff-avg [L/s]" = "Qeff-avg",      # Average effluent flow rate
"n_inf [-]" = "n_inf",               # Number of influent samples
"n_eff [-]" = "n_eff") %>%            # Number of effluent samples
slice(-1) %>%  # Remove the row containing units
select(-Event) # Remove the event count column
# Read in RainMeanCharacteristics1.csv
rain_chara_1.df <- read_csv("Data/commaSeparatedValues/RainMeanCharacteristics1.csv",
col_types = cols(.default = col_character())) %>% # Read in the file
# Rename columns to include units
rename("Date" = "Rainfall/Runoff event",   # Date for ease of joining later
"Cl(-) [ug/L]" = "Cl(-)",           # Chloride concentration
"SO4(2-) [ug/L]" = "SO4 (2-)",      # Sulfate concentration
"PO4(3-) [ug/L]" = "PO4 (3-)",      # Phosphate concentration
"NH3-N [ug/L]" = "NH3-N",           # Ammonia concentration
"NO3(-)-N [ug/L]" = "NO3(-)-N",     # Nitrate concentration
"TDN [ug/L]" = "TDN",               # Total dissolved nitrogen
"COD [mg/L]" = "COD",               # Chemical oxygen demand
"Turbidity [NTU]" = "Turbidity",    # Turbidity
"NO2(-)-N [ug/L]" = "NO2(-)-N") %>% # Nitrite concentration
slice(-1) # Remove the row containing units
# Read in RainMeanCharacteristics2.csv
rain_chara_2.df <- read_csv("Data/commaSeparatedValues/RainMeanCharacteristics2.csv",
col_types = cols(.default = col_character())) %>%  # Units were already included in top row, so full renaming is not necessary
rename("Date" = `Rainfall/Runoff event`)
rain_chara_full.df <- full_join(rain_chara_1.df, rain_chara_2.df, by = "Date")
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(matches("^Avg"))
View(inf_phys_chem_avg.df)
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) #%>% # Reads the data set
names(inf_phys_chem_avg.df)
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) #%>% # Reads the data set
names(inf_phys_chem_avg.df)
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(matches("^Avg"))
inf_phys_chem_avg.df <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>% # Reads the data set
select(`Rainfall/Runoff event`, matches("^Avg"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Rainfall/Runoff Characteristics
RF_RO_chara.df <- read_csv('Data/commaSeparatedValues/RF_RO_Event_Characteristics.csv',
col_types = cols(.default = col_character())) %>%
# Rename columns to include units
rename("PHD [hr]" = "PHD",                  # Previous dry hours
"d_rain [min]" = "d_rain",           # Duration of the rain event
"h_rain [in]" = "h_rain",            # Depth of the rain event
"i_rain-max [mm/hr]" = "i_rain-max", # Rainfall maximum intensity
"IPRT [min]" = "IPRT",               # Initial pavement residence time
"V_runoff [L]" = "V_runoff",         # Volume of runoff
"C [-]" = "C",                       # Runoff coefficient
"Q_inf-max [L/s]" = "Qinf-max",      # Maximum influent flow rate
"Q_inf-avg [L/s]" = "Qinf-avg",      # Average influent flow rate
"Q_eff-max [L/s]" = "Qeff-max",      # Maximum effluent flow rate
"Q_eff-avg [L/s]" = "Qeff-avg",      # Average effluent flow rate
"n_inf [-]" = "n_inf",               # Number of influent samples
"n_eff [-]" = "n_eff") %>%            # Number of effluent samples
slice(-1) %>%  # Remove the row containing units
select(-Event) # Remove the event count column
# Read in RainMeanCharacteristics1.csv
rain_chara_1.df <- read_csv("Data/commaSeparatedValues/RainMeanCharacteristics1.csv",
col_types = cols(.default = col_character())) %>% # Read in the file
# Rename columns to include units
rename("Date" = "Rainfall/Runoff event",   # Date for ease of joining later
"Cl(-) [ug/L]" = "Cl(-)",           # Chloride concentration
"SO4(2-) [ug/L]" = "SO4 (2-)",      # Sulfate concentration
"PO4(3-) [ug/L]" = "PO4 (3-)",      # Phosphate concentration
"NH3-N [ug/L]" = "NH3-N",           # Ammonia concentration
"NO3(-)-N [ug/L]" = "NO3(-)-N",     # Nitrate concentration
"TDN [ug/L]" = "TDN",               # Total dissolved nitrogen
"COD [mg/L]" = "COD",               # Chemical oxygen demand
"Turbidity [NTU]" = "Turbidity",    # Turbidity
"NO2(-)-N [ug/L]" = "NO2(-)-N") %>% # Nitrite concentration
slice(-1) # Remove the row containing units
# Read in RainMeanCharacteristics2.csv
rain_chara_2.df <- read_csv("Data/commaSeparatedValues/RainMeanCharacteristics2.csv",
col_types = cols(.default = col_character())) %>%  # Units were already included in top row, so full renaming is not necessary
rename("Date" = `Rainfall/Runoff event`)
rain_chara_full.df <- full_join(rain_chara_1.df, rain_chara_2.df, by = "Date")
# Import the individual influent data sheets, then combine them into one single data frame
inf_phys_chem <- read_csv("Data/commaSeparatedValues/InfluentCharacteristicsPhysChem.csv",
col_types = cols(.default = col_character())) %>%
rename("Date" = `Rainfall/Runoff event`)
View(inf_phys_chem)
names(inf_phys_chem)
install.packages("neuralnet")
library(tidyverse)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("BaffleMLData.csv", col_types = cols(.default = col_number()))
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
View(data)
data <- data[ ,6:10]
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(efficiency~., data=train) # fitting reg linear regression model
summary(lm.fit)
# summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$efficiency)^2)/nrow(test) # MSE is mean squared error
pr.lm
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ <- scaled[index,]
test_ <- scaled[-index,]
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$cost)^2)/nrow(test) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ <- scaled[index,]
test_ <- scaled[-index,]
n <- names(train_)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$cost)-min(data$cost))+min(data$cost)
test.r <- (test_$cost)*(max(data$cost)-min(data$cost))+min(data$cost)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
print(paste0("Linear regression MSE: ",MSE.lm," and neural network MSE: ",MSE.nn))
# Since LM MSE is less than NN MSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$efficiency)^2)/nrow(test) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
test.r <- (test_ann$efficiency)*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_ann)
print(paste0("Linear regression MSE: ",MSE.lm," and neural network MSE: ",MSE.nn))
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- combined %>%
mutate(nn_efficiency = c(test.r))
write.csv(combined, "MLPredictedData.csv")
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm <- predict(lm.fit,test)
RMSE.lm <- sqrt(sum((pr.lm - test$cost)^2)/nrow(test)) # RMSE is root mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ <- scaled[index,]
test_ <- scaled[-index,]
n <- names(train_)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$cost)-min(data$cost))+min(data$cost)
test.r <- (test_$cost)*(max(data$cost)-min(data$cost))+min(data$cost)
RMSE.nn <- sqrt(sum((test.r - pr.nn_)^2)/nrow(test_))
print(paste0("Linear regression MSE: ",RMSE.lm," and neural network MSE: ",RMSE.nn))
# Since LM MSE is less than NN MSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$efficiency)^2)/nrow(test) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
test.r <- (test_ann$efficiency)*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
RMSE.nn <- sqrt(sum((test.r - pr.nn_)^2)/nrow(test_ann))
print(paste0("Linear regression MSE: ",RMSE.lm," and neural network MSE: ",RMSE.nn))
# Since NN MSE is less than LM MSE, we will proceed with NN for efficiency
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- combined %>%
mutate(nn_efficiency = c(test.r))
write.csv(combined, "Data/MLPredictedData.csv")
write.csv(combined, "MLPredictedData.csv")
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm <- predict(lm.fit,test)
RMSE.lm <- sqrt(sum((pr.lm - test$cost)^2)/nrow(test)) # RMSE is root mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ <- scaled[index,]
test_ <- scaled[-index,]
n <- names(train_)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$cost)-min(data$cost))+min(data$cost)
test.r <- (test_$cost)*(max(data$cost)-min(data$cost))+min(data$cost)
RMSE.nn <- sqrt(sum((test.r - pr.nn_)^2)/nrow(test_))
print(paste0("Linear regression MSE: ",RMSE.lm," and neural network MSE: ",RMSE.nn))
# Since LM MSE is less than NN MSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm,col='blue',main='Real cost vs predicted cost lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm <- predict(lm.fit,test)
RMSE.lm <- sqrt(sum((pr.lm - test$cost)^2)/nrow(test)) # RMSE is root mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$cost)-min(data$cost))+min(data$cost)
test.r <- (test_ann$cost)*(max(data$cost)-min(data$cost))+min(data$cost)
RMSE.nn <- sqrt(sum((test.r - pr.nn_)^2)/nrow(test_ann))
print(paste0("Linear regression MSE: ",RMSE.lm," and neural network MSE: ",RMSE.nn))
# Since LM MSE is less than NN MSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm,col='blue',main='Real cost vs predicted cost lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$efficiency)^2)/nrow(test) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
plot(nn)
pr.nn <- compute(nn,test_[,1:4])
pr.nn_ <- pr.nn$net.result*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
test.r <- (test_ann$efficiency)*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
RMSE.nn <- sqrt(sum((test.r - pr.nn_)^2)/nrow(test_ann))
print(paste0("Linear regression MSE: ",RMSE.lm," and neural network MSE: ",RMSE.nn))
# Since NN MSE is less than LM MSE, we will proceed with NN for efficiency
setseed(500)
set.seed(500)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
