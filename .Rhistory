points(test$cost,pr.lm,col='blue',pch=18,cex=0.7)
par(mfrow=c(1,2))
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm.cost,col='blue',main='Real cost vs predicted cost lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
points(test$cost,pr.lm.cost,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm)))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm.cost)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data_eff),round(0.75*nrow(data_eff)))
train <- data_eff[index,]
test <- data_eff[-index,]
lm.eff <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm <- predict(lm.eff,test)
RMSE.lm.eff <- sqrt(sum((pr.lm - test$efficiency)^2)/nrow(test)) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_eff, 2, max)
mins <- apply(data_eff, 2, min)
scaled <- as.data.frame(scale(data_eff, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn.eff <- neuralnet(f,data=train_ann,hidden=c(5,3),linear.output=T)
plot(nn)
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_eff, 2, max)
mins <- apply(data_eff, 2, min)
scaled <- as.data.frame(scale(data_eff, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn.eff <- neuralnet(f,data=train_ann,hidden=c(5,3),linear.output=T)
plot(nn.eff)
pr.nn.eff <- compute(nn.eff,test_ann[,1:4])
pr.nn_.eff <- pr.nn.eff$net.result*(max(data_eff$efficiency)-min(data_eff$efficiency))+min(data_eff$efficiency)
test.r <- (test_ann$efficiency)*(max(data_eff$efficiency)-min(data_eff$efficiency))+min(data_eff$efficiency)
RMSE.nn.eff <- sqrt(sum((test.r - pr.nn_.eff)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.eff," and neural network RMSE: ",RMSE.nn.eff))
# Since NN MSE is less than LM MSE, we will proceed with NN for efficiency
set.seed(500)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm.eff,col='blue',main='Real vs predicted efficiency lm',pch=18, cex=0.7, xlim = c(99.85,99.95))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data_eff),round(0.75*nrow(data_eff)))
train <- data_eff[index,]
test <- data_eff[-index,]
lm.eff <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm.eff <- predict(lm.eff,test)
RMSE.lm.eff <- sqrt(sum((pr.lm - test$efficiency)^2)/nrow(test)) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_eff, 2, max)
mins <- apply(data_eff, 2, min)
scaled <- as.data.frame(scale(data_eff, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn.eff <- neuralnet(f,data=train_ann,hidden=c(5,3),linear.output=T)
plot(nn.eff)
pr.nn.eff <- compute(nn.eff,test_ann[,1:4])
pr.nn_.eff <- pr.nn.eff$net.result*(max(data_eff$efficiency)-min(data_eff$efficiency))+min(data_eff$efficiency)
test.r <- (test_ann$efficiency)*(max(data_eff$efficiency)-min(data_eff$efficiency))+min(data_eff$efficiency)
RMSE.nn.eff <- sqrt(sum((test.r - pr.nn_.eff)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.eff," and neural network RMSE: ",RMSE.nn.eff))
# Since NN MSE is less than LM MSE, we will proceed with NN for efficiency
set.seed(500)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm.eff,col='blue',main='Real vs predicted efficiency lm',pch=18, cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
points(test$efficiency,pr.lm.eff,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- combined %>%
mutate(nn_efficiency = c(test.r))
write.csv(combined, "MLPredictedData.csv")
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_cost_pr <- basin_test
basin_cost_pr$cost <- predict(lm.cost, basin_test)
View(basin_cost_pr)
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_cost_pr <- basin_test
basin_cost_pr$cost <- predict(lm.cost, basin_test)
basin_eff_pr <- basin_test
basin_eff_pr$efficiency <- predict(nn.eff, basin_test)
View(basin_eff_pr)
View(basin_cost_pr)
View(basin_eff_pr)
names(basin_eff_pr)
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
data_cost <- data %>% # Create data set to train model for cost prediction
select(-efficiency)
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data_cost),round(0.75*nrow(data_cost)))
train <- data_cost[index,]
test <- data_cost[-index,]
lm.cost <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm.cost <- predict(lm.cost,test)
RMSE.lm.cost <- sqrt(sum((pr.lm.cost - test$cost)^2)/nrow(test)) # RMSE is root mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_cost, 2, max)
mins <- apply(data_cost, 2, min)
scaled <- as.data.frame(scale(data_cost, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn.cost <- neuralnet(f,data=train_ann,hidden=c(3,linear.output=T))
plot(nn.cost)
pr.nn.cost <- compute(nn.cost,test_ann[,1:4])
pr.nn_.cost <- pr.nn.cost$net.result*(max(data_cost$cost)-min(data_cost$cost))+min(data_cost$cost)
test.r <- (test_ann$cost)*(max(data_cost$cost)-min(data_cost$cost))+min(data_cost$cost)
RMSE.nn.cost <- sqrt(sum((test.r - pr.nn_.cost)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.cost," and neural network RMSE: ",RMSE.nn.cost))
# Since LM RMSE is less than NN RMSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm.cost,col='blue',main='Real cost vs predicted cost lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
points(test$cost,pr.lm.cost,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm.cost)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data_eff),round(0.75*nrow(data_eff)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.eff <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm.eff <- predict(lm.eff,test)
RMSE.lm.eff <- sqrt(sum((pr.lm - test$efficiency)^2)/nrow(test)) # MSE is mean squared error
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.eff <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm.eff <- predict(lm.eff,test)
RMSE.lm.eff <- sqrt(sum((pr.lm.eff - test$efficiency)^2)/nrow(test)) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_eff, 2, max)
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn.eff <- neuralnet(f,data=train_ann,hidden=c(5,3),linear.output=T)
plot(nn.eff)
pr.nn.eff <- compute(nn.eff,test_ann[,1:4])
pr.nn_.eff <- pr.nn.eff$net.result*(max(data_eff$efficiency)-min(data_eff$efficiency))+min(data_eff$efficiency)
pr.nn.eff <- compute(nn.eff,test_ann[,1:4])
pr.nn_.eff <- pr.nn.eff$net.result*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
test.r <- (test_ann$efficiency)*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
RMSE.nn.eff <- sqrt(sum((test.r - pr.nn_.eff)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.eff," and neural network RMSE: ",RMSE.nn.eff))
# Since NN MSE is less than LM MSE, we will proceed with NN for efficiency
set.seed(500)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm.eff,col='blue',main='Real vs predicted efficiency lm',pch=18, cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
points(test$efficiency,pr.lm.eff,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- combined %>%
mutate(nn_efficiency = c(test.r))
write.csv(combined, "MLPredictedData.csv")
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_cost_pr <- basin_test
basin_cost_pr$cost <- predict(lm.cost, basin_test)
basin_eff_pr <- basin_test
basin_eff_pr$efficiency <- predict(nn.eff, basin_test)
basin_pr$cost <- predict(basin_test, lm.cost)
basin_pr$cost <- predict(lm.cost, basin_test)
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_pr <- basin_test
basin_pr$cost <- predict(lm.cost, basin_test)
View(basin_pr)
basin_pr$efficiency <- predict(nn.eff, basin_pr)
View(data)
View(nn.eff)
View(pr.nn.eff)
?compute
basin_pr$efficiency <- compute(nn.eff, basin_pr)
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_pr <- basin_test
basin_pr$cost <- predict(lm.cost, basin_test)
basin_pr$efficiency <- compute(nn.eff, basin_pr)
View(basin_pr)
basin_pr$efficiency <- prediction(nn.eff, basin_pr)
basin_pr$efficiency <- predict(nn.eff, basin_pr)
View(basin_pr)
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_pr <- basin_test
basin_pr$cost <- predict(lm.cost, basin_test)
basin_pr$efficiency <- predict(nn.eff, basin_pr)
# run in console:
# install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
# library(MASS) # data for testing
library(neuralnet)
library(ggplot2)
library(dplyr)
data_raw <- read_csv("../Data/BaffleMLData.csv", col_types = cols(.default = col_number()))
data <- data_raw %>%
mutate(length = `Basin Length (ft)`,
width = `Basin Width (ft)`,
baffle_num = `Number of Baffles`,
efficiency = `PM Separation Efficiency (%)`,
cost = `Cost($)`)
data <- data[ ,6:10]
data_cost <- data %>% # Create data set to train model for cost prediction
select(-efficiency)
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
set.seed(500)
index <- sample(1:nrow(data_cost),round(0.75*nrow(data_cost)))
train <- data_cost[index,]
test <- data_cost[-index,]
lm.cost <- glm(cost~., data=train) # fitting reg linear regression model
#summary(lm.fit)
pr.lm.cost <- predict(lm.cost,test)
RMSE.lm.cost <- sqrt(sum((pr.lm.cost - test$cost)^2)/nrow(test)) # RMSE is root mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data_cost, 2, max)
mins <- apply(data_cost, 2, min)
scaled <- as.data.frame(scale(data_cost, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("cost ~", paste(n[!n %in% "cost"], collapse = " + ")))
nn.cost <- neuralnet(f,data=train_ann,hidden=c(3,linear.output=T))
plot(nn.cost)
pr.nn.cost <- compute(nn.cost,test_ann[,1:4])
pr.nn_.cost <- pr.nn.cost$net.result*(max(data_cost$cost)-min(data_cost$cost))+min(data_cost$cost)
test.r <- (test_ann$cost)*(max(data_cost$cost)-min(data_cost$cost))+min(data_cost$cost)
RMSE.nn.cost <- sqrt(sum((test.r - pr.nn_.cost)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.cost," and neural network RMSE: ",RMSE.nn.cost))
# Since LM RMSE is less than NN RMSE, we will proceed with LM for cost
par(mfrow=c(1,2))
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$cost,pr.lm.cost,col='blue',main='Real cost vs predicted cost lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$cost,pr.nn_.cost,col='red',main='Real cost vs predicted cost NN',pch=18,cex=0.7)
points(test$cost,pr.lm.cost,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- as.data.frame(test[ ,1:3]) %>%
mutate(lm_cost = c(as.numeric(pr.lm.cost)))
#sampledata <- Boston
#apply(data,2,function(x) sum(is.na(x))) # run to make sure no missing data
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.eff <- glm(efficiency~., data=train) # fitting reg linear regression model
# summary(lm.fit)
pr.lm.eff <- predict(lm.eff,test)
RMSE.lm.eff <- sqrt(sum((pr.lm.eff - test$efficiency)^2)/nrow(test)) # MSE is mean squared error
# Randomly selecting rows to assign to be part of training or testing data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins)) # normalizing data!!!
train_ann <- scaled[index,]
test_ann <- scaled[-index,]
n <- names(train_ann)
f <- as.formula(paste("efficiency ~", paste(n[!n %in% "efficiency"], collapse = " + ")))
nn.eff <- neuralnet(f,data=train_ann,hidden=c(5,3),linear.output=T)
plot(nn.eff)
pr.nn.eff <- compute(nn.eff,test_ann[,1:4])
pr.nn_.eff <- pr.nn.eff$net.result*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
test.r <- (test_ann$efficiency)*(max(data$efficiency)-min(data$efficiency))+min(data$efficiency)
RMSE.nn.eff <- sqrt(sum((test.r - pr.nn_.eff)^2)/nrow(test_ann))
print(paste0("Linear regression RMSE: ",RMSE.lm.eff," and neural network RMSE: ",RMSE.nn.eff))
# Since NN RMSE is less than LM RMSE, we will proceed with NN for efficiency
set.seed(500)
par(mfrow=c(1,2))
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$efficiency,pr.lm.eff,col='blue',main='Real vs predicted efficiency lm',pch=18, cex=0.7, xlim = c(99.85,99.95))
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$efficiency,pr.nn_.eff,col='red',main='Real vs predicted efficiency NN',pch=18,cex=0.7, xlim = c(99.85,99.95))
points(test$efficiency,pr.lm.eff,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
combined <- combined %>%
mutate(nn_efficiency = c(test.r))
write.csv(combined, "MLPredictedData.csv")
basin_test <- data.frame(length = rep(seq(140, 160, by = 1), each = 6),
width = rep(0,each = 126),
baffle_num = rep(c(0,1,3,5,7,9), times = 21)
)%>%
mutate(width = length/10)
basin_pr <- basin_test
basin_pr$cost <- predict(lm.cost, basin_test)
basin_pr$efficiency <- predict(nn.eff, basin_pr)
knitr::opts_chunk$set(echo = TRUE)
read.csv("GNV_15min_precip.csv")
?read_csv
??read_csv
library(tidyverse)
library(tidyverse)
library(dplyr)
data_raw <- read_csv("GNV_15min_precip.csv")
data_raw <- read_csv("GNV_15min_precip.csv")
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv")
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.defaul = col_number()))
View(data_raw)
names(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.defaul = col_number())) %>%
mutate("...8" = "TIME")
View(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.defaul = col_number())) %>%
mutate(TIME = `...8`)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_number())) %>%
mutate(TIME = `...8`)
problems(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_number())) %>%
mutate(TIME = `...8`)
?problems()
problems(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_number()))
problems(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_number()))
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_number()))
problems(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
mutate(precip = )
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character()))
ksadfj
names(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character()))
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
mutate(precip = QPCP)
hi
View(data_raw)
?reaname
?rename
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP)
names(data_raw)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')))
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')), # Extract the year from the date
date = str_replace(date, '\\d{4}-'))
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')), # Extract the year from the date
date = str_replace(date, '\\d{4}-',''))
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')), # Extract the year from the date into new column
date = str_replace(date, '\\d{4}-','')) # Remove the year portion of the dates from the date column
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')), # Extract the year from the date into new column
date = str_replace(date, '\\d{4}-','')) # Remove the year portion of the dates from the date column
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')), # Extract the year from the date into new column
date = str_replace(date, '\\d{4}','')) # Remove the year portion of the dates from the date column
?str_replace
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character())) %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')),  # Extract the year from the date into new column
date = str_replace(date, '\\d{4}',''),          # Remove the year portion of the dates from the date column
month = as.numeric(str_extract(date,'\\d{2}')), # Extract the year from the date into a new column
date = str_replace(date, '\\d{2}',''))
?max
max(data_raw$month)
max(data_raw$date)
data_raw <- read_csv("Data/SWMMStuff/GNV_15min_precip.csv",
col_types = cols(.default = col_character()))
data_mod <- data_raw %>%
rename(precip = QPCP,
date = DATE,
time = TIME) %>%
mutate(year = as.numeric(str_extract(date,'\\d{4}')),  # Extract the year from the date into new column
date = str_replace(date, '\\d{4}',''),          # Remove the year portion of the dates from the date column
month = as.numeric(str_extract(date,'\\d{2}')), # Extract the year from the date into a new column
date = str_replace(date, '\\d{2}',''),          # Remove the month portion
day = as.numeric(date)) %>%                     # Make a new column with the days
select(month, day, year, time, precip)
View(data_mod)
