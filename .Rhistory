splitrule = c("gini", "extratrees"), # Extratrees is a splitting rule for the decision trees that splits the nodes by choosing cut-points fully at random, and gini is a splitting rule that looks at the probability the model is wrong
min.node.size = 1 # Default is 1 for classification
)
# Create a grid to use when creating the model
my_grid <- expand.grid(mtry = c(5, 10, 20, 40, 60), # Create a data frame, and define the number of variable randomly collected to sample for each split
splitrule = c("gini", "extratrees"), # Extratrees is a splitting rule for the decision trees that splits the nodes by choosing cut-points fully at random, and gini is a splitting rule that looks at the probability the model is wrong
min.node.size = 1 # Default is 1 for classification
)
library("rpart") # Recursive partitioning package
library("rpart.plot") # Plotting recursive partioning package
library("mlbench") # Sonar data being used
library("caret") # Contains the train function
# Plotting the first decision tree randomly made
data(Sonar)
set.seed(42) # Used in practice to judge results and how they relate to the method given by tutorial
# Create a grid to use when creating the model
my_grid <- expand.grid(mtry = c(5, 10, 20, 40, 60), # Create a data frame, and define the number of variable randomly collected to sample for each split
splitrule = c("gini", "extratrees"), # Extratrees is a splitting rule for the decision trees that splits the nodes by choosing cut-points fully at random, and gini is a splitting rule that looks at the probability the model is wrong
min.node.size = 1 # Default is 1 for classification
)
View(my_grid)
# Create the model used
model <- train(Class ~ .,
data = Sonar,
method = "ranger", # Random forest method to create a model
tuneGrid = myGrid,
trControl = trainControl(method = "cv", # Resampling method; cross validation
number = 5, # Number of folds for cross validation
verboseIter = FALSE # Won't print the training log
)
# tuneLength = 5 # Sets number of hyperparameter values to test; commented out for practice elsewhere
)
# Create the model used
model <- train(Class ~ .,
data = Sonar,
method = "ranger", # Random forest method to create a model
tuneGrid = my_grid,
trControl = trainControl(method = "cv", # Resampling method; cross validation
number = 5, # Number of folds for cross validation
verboseIter = FALSE # Won't print the training log
)
# tuneLength = 5 # Sets number of hyperparameter values to test; commented out for practice elsewhere
)
print(model) # Provide information about the model
plot(model) # Graphically represent the model's accuracy
# Create the model used
model <- train(Class ~ .,
data = Sonar,
method = "ranger", # Random forest method to create a model
tuneGrid = my_grid,
trControl = trainControl(method = "cv", # Resampling method; cross validation
number = 5, # Number of folds for cross validation
verboseIter = FALSE # Won't print the training log
)
tuneLength = 5 # Sets number of hyperparameter values to test; commented out for practice elsewhere
# Create the model used
model <- train(Class ~ .,
data = Sonar,
method = "ranger", # Random forest method to create a model
tuneGrid = my_grid,
trControl = trainControl(method = "cv", # Resampling method; cross validation
number = 5, # Number of folds for cross validation
verboseIter = FALSE # Won't print the training log
),
tuneLength = 5 # Sets number of hyperparameter values to test; commented out for practice elsewhere
)
print(model) # Provide information about the model
plot(model) # Graphically represent the model's accuracy
install.packages("caTools") # Contains the colAUC function which will be used later on
library("caTools")
?trainControl
??twoClassSummary
??classProbs
?trainControl
data(Sonar)
library("caTools")
library("caret")   # Contains 'train' function used later
library("mlbench") # Contains Sonar data being used
data(Sonar)
# Create the trainControl
my_control <- trainControl(method = "cv", # Cross validation
number = 10, # 10 folds
summaryFunction = twoClassSummary, # Computes sensitivity, specificity, and AUC for the ROC curve
classProbs = TRUE, # Recalculates class probabilities in each resample
verboseIter = FALSE # Suppresses dialogue with iterations
)
# Train the glm with trainControl model
model <- train(Class ~ .,
Sonar,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
warnings()
# Train the glm with trainControl model
model <- train(Class ~ .,
Sonar,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
library("caTools")
library("caret")   # Contains 'train' function used later
library("mlbench") # Contains Sonar data being used
data(Sonar)
# Create the trainControl
my_control <- trainControl(method = "cv", # Cross validation
number = 10, # 10 folds
summaryFunction = twoClassSummary, # Computes sensitivity, specificity, and AUC for the ROC curve
classProbs = TRUE, # Recalculates class probabilities in each resample
verboseIter = FALSE # Suppresses dialogue with iterations
)
# Train the glm with trainControl model
model <- train(Class ~ .,
Sonar,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
plot(model)
# Pint model to console
print(model)
?glm
# Building up similar to the confusion matrix method
tr <- sample(nrow(Sonar), round(nrow(Sonar)*.6)) # Number of training samples
train <- Sonar[tr,] # Training samples
test <- Sonar[-tr,] # Testing samples
model_graph <- glm(Class ~ ., # create the model
data = train, # Training data
family = "binomial") # error distribution type
p <- predict(model_graph, # Use the model developed above
test,  # Use the test data to check accuracy
type = "response")
summary(p)
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create the trainControl
my_control <- trainControl(method = "cv", # Cross validation
number = 10, # 10 folds
summaryFunction = twoClassSummary, # Computes sensitivity, specificity, and AUC for the ROC curve
classProbs = TRUE, # Recalculates class probabilities in each resample
verboseIter = FALSE # Suppresses dialogue with iterations
)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.6))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "response")
p <- predict(model, # Use the model developed above
test  # Use the test data to check accuracy
)
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
?predict
??type
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "response")
install.packages("caTools") # Contains the colAUC function which will be used later on
install.packages("caTools")
library("caTools")
library("caret")   # Contains 'train' function used later
library("mlbench") # Contains Sonar data being used
data(Sonar)
# Create the trainControl
my_control <- trainControl(method = "cv", # Cross validation
number = 10, # 10 folds
summaryFunction = twoClassSummary, # Computes sensitivity, specificity, and AUC for the ROC curve
classProbs = TRUE, # Recalculates class probabilities in each resample
verboseIter = FALSE # Suppresses dialogue with iterations
)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.6))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create the trainControl
my_control <- trainControl(method = "cv", # Cross validation
number = 20, # 10 folds
summaryFunction = twoClassSummary, # Computes sensitivity, specificity, and AUC for the ROC curve
classProbs = TRUE, # Recalculates class probabilities in each resample
verboseIter = FALSE # Suppresses dialogue with iterations
)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.6))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.9))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.20))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.50))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.60))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
# Create training and testing data
tr <- sample(nrow(Sonar), round(nrow(Sonar)*0.60))
train <- Sonar[tr, ] # Creates the training set
test <- Sonar[-tr, ] # Creates the testing
# Train the glm with trainControl model
model <- train(Class ~ .,
train,
method = "glm", # Use the Generalized linear regression method
trControl = my_control
)
# Pint model to console
print(model)
p <- predict(model, # Use the model developed above
test,  # Use the test data to check accuracy
type = "prob")
summary(p)
# Use the colAUC function to visualize results
caTools::colAUC(p, test[["Class"]], plotROC = TRUE)
install.package("C50")
install.packages("C50")
library("C50")
data(churn)
table(churnTrain$churn)/nrow(churnTrain)
?churn
??churn
table(churnTrain$churn)/nrow(churnTrain)
data(mlc_churn)
library("modeldata")
install.packages("modeldata")
library("modeldata")
library("C50")
data(mlc_churn)
data(churn)
table(churnTrain$churn)/nrow(churnTrain)
force(mlc_churn)
View(mlc_churn)
table(churnTrain$mlc_churn)/nrow(churnTrain)
?churnTrain
??churnTrain
?churn
?mlc_churn
churnTrain$mlc_churn
View(mlc_churn)
churn <- data(mlc_churn) # Loads in the data used for this practic
tr <- sample(nrow(churn),
nrow(churn)*samp_perc)
churn <- data(mlc_churn) # Loads in the data used for this practic
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(churn),
nrow(churn)*samp_perc)
tr <- sample(nrow(churn),
round(nrow(churn)*samp_perc))
nrow(churn)*samp_perc
nrow(churn)
churn <- data(mlc_churn) # Loads in the data used for this practic
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(churn),
round(nrow(churn)*samp_perc))
data(mlc_churn) # Loads in the data used for this practic
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- churn[tr, ] # Create the training set
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(churnTrain$mlc_churn)/nrow(churnTrain)
table(churn$mlc_churn)/nrow(churnTrain)
table(churn$mlc_churn)/nrow(mlc_churn)
library("modeldata") # Contains the 'mlc_churn' data used for practice
library("C50")
data(mlc_churn) # Loads in the data used for this practic
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(churn$mlc_churn)/nrow(mlc_churn)
churn$mlc_churn
table(mlc_churn$churn)/nrow(mlc_churn)
table(churnTrain$churn)/nrow(churnTrain)
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
set.seed(42) # Set a specific seed to prevent randomness generating a new set of samples
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
set.seed(43) # Set a specific seed to prevent randomness generating a new set of samples
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
str(my_folds)
my_folds <- createFolds(churnTrain$churn, k = 5)
str(my_folds)
table(my_folds[1])/nrow(my_folds)
str(my_folds) # lists out the folds
my_folds <- createFolds(churnTrain$churn,
k = 5) # Defines the number of folds
str(my_folds) # lists out the folds
table(my_folds[1])/nrow(my_folds)
?sapply
?function
help
sapply(my_folds, function(i){
table(churnTrain$churn[i])/length(i)
})
?trainControl
?train
library("caret")
my_control <- trainControl(summaryFunction = twoClassSummary, # The output of the training will have two different options
classProb = TRUE, # recalculate class probabilities for each re-sample
savePredictions = TRUE, # All hold-out predictions for each re-sample should be saved
index = my_folds # Lists each element for each re-sampling iteration
)
?expand.grid
install.packages("glmnet")
install.packages("Matrix")
install.packages("Matrix")
install.packages
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
install.packages("Matrix")
library("modeldata") # Contains the 'mlc_churn' data used for practice
library("C50") # Contains churn data
library("caret") # Using the 'train' function
library("glmnet") # Required to use the glmnet method
library("Matrix") # Required to use the glmnet method
data(mlc_churn) # Loads in the data used for this practic
set.seed(43) # Set a specific seed to prevent randomness generating a new set of samples
samp_perc <- 0.6 # Defines the percent of the sample used for training
tr <- sample(nrow(mlc_churn),
round(nrow(mlc_churn)*samp_perc))
churnTrain <- mlc_churn[tr, ] # Create the training set
churnTest <- mlc_churn[-tr, ] # Create the testing set
table(mlc_churn$churn)/nrow(mlc_churn) # Percent of churn in the original data
table(churnTrain$churn)/nrow(churnTrain) # percent of churn in the training data, should be close to 14.14% (percent of churn in full data set)
my_folds <- createFolds(churnTrain$churn,
k = 5) # Defines the number of folds
str(my_folds) # lists out the folds
sapply(my_folds, function(i){ # Verify that the folds have a similar proportion of the yes/no results
table(churnTrain$churn[i])/length(i)
})
my_control <- trainControl(summaryFunction = twoClassSummary, # The output of the training will have two different options
classProb = TRUE, # recalculate class probabilities for each re-sample
savePredictions = TRUE, # All hold-out predictions for each re-sample should be saved
index = my_folds # Lists each element for each re-sampling iteration
)
# Create the glm model
glm_model <- train(churn ~ ., # Determines the output
churnTrain, # All of the training data
metric = "ROC", # Use ROC to determine which classification threshold is best
method = "glmnet", # Use a generelized linear regression method for this model
tuneGrid = expand.grid( # Data frame with tuning values
alpha = 0:1,# One of the tuning parameters for glmnet, mixing percentage
lambda = 0:10/10), # The other tuning parameter for glmnet, regularization parameter
trControl = my_control # Sets training controls to pre-established parameters
)
print(glm_model)
plot(glm_model)
# Developing a random forest model
rf_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # Same metric as before
method = "ranger", # Random forest method
tuneGrid = expand.grid(
mtry = c(2, 5, 10, 19), # Number of randomly selected predictors, max being 20
splitrule = c("gini", "extratrees"), # Defines the splitting rules as gini and extra trees
min.node.size = 1 # Sets minimum node size to 1
),
trControl = my_control # Sets the training controls as the pre-established parameters
)
print(rf_model)
plot(rf_model)
?train
# Developing a kNN model
knn_model <- train(churn ~ .,
churnTrain,
metric = "ROC", # same metric as before
method = "knn", # kNN method
tuneLength = 20, # Number of tuning parameters, 20 bc the data contains 20 columns
trControl = my_control)
print(knn_model)
plot(knn_model)
# run in console:
#install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
library(tidyverse)
library(MASS) # data
library(neuralnet)
library(ggplot2)
